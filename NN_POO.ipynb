{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss_type = LossType()\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, A):\n",
    "        for layer in self.layers:\n",
    "            A = layer.forward(A)\n",
    "        return A\n",
    "    \n",
    "    def backward_pass(self, dERROR, learning_rate, multiclass = False):\n",
    "        if multiclass == True:\n",
    "            #all layers without softmax\n",
    "            layers = self.layers[0:-1]\n",
    "        else:\n",
    "            layers = self.layers\n",
    "        \n",
    "        for layer in reversed(layers):\n",
    "            dERROR = layer.backward(dERROR, learning_rate)\n",
    "            \n",
    "    def get_accuracy(self, A, Y):\n",
    "        preds = np.round(A, decimals=0)\n",
    "        results = (Y == preds)\n",
    "        results = np.squeeze(np.sum(results, axis = 1, keepdims=True))\n",
    "        #numÃ©ro d'exemples\n",
    "        m = A.shape[1]\n",
    "        rate = results / m\n",
    "        return rate\n",
    "                \n",
    "    def train(self, X, Y, learning_rate, epochs=20, multiclass = False):\n",
    "        \n",
    "        for e in range(epochs): \n",
    "            A = self.forward(X)\n",
    "            dERROR = A - Y\n",
    "            \n",
    "            if multiclass == True:\n",
    "                if e % 500 == 0: print(\"Epoch:\", e, \"Cost:\", self.loss_type.multiclass(A, Y))\n",
    "            else:\n",
    "                if e % 500 == 0:\n",
    "                    rate = self.get_accuracy(A,Y)\n",
    "                    print(\"Epoch:\", e, \"Cost:\", self.loss_type.binary(A, Y), \"Acc:\", rate)\n",
    "                \n",
    "            self.backward_pass(dERROR, learning_rate, multiclass=multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossType():\n",
    "    \n",
    "    def binary(self, A, Y):\n",
    "        loss = - (np.log(A) * Y + np.log(1 - A) * (1 - Y))\n",
    "        m = A.shape[1]\n",
    "        cost = np.sum(loss, axis=1, keepdims=True) / m\n",
    "        cost = np.round(np.squeeze(cost), 3)\n",
    "        return cost\n",
    "    \n",
    "    def multiclass(self,Y,A):\n",
    "        YL=np.multiply(A,Y)\n",
    "        YL=YL[YL!=0]\n",
    "        YL=-np.log(YL)\n",
    "        YL=np.mean(YL)\n",
    "        return YL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerFC():\n",
    "    \n",
    "    def __init__(self, n_x, n_h):\n",
    "        self.W = np.random.randn(n_h, n_x)\n",
    "        self.b = np.zeros((n_h, 1))\n",
    "    \n",
    "    def forward(self, A_prev):\n",
    "        self.A_prev = A_prev\n",
    "        self.Z = np.dot(self.W, self.A_prev) + self.b\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dERROR, learning_rate):\n",
    "        m = dERROR.shape[1]\n",
    "        dW = (1 / m) * np.dot(dERROR, self.A_prev.T)\n",
    "        db = (1 / m) * np.sum(dERROR, axis=1, keepdims=True)\n",
    "        dERROR = np.dot(self.W.T, dERROR)\n",
    "        \n",
    "        self.W = self.W - learning_rate * dW        \n",
    "        self.b = self.b - learning_rate * db        \n",
    "        \n",
    "        return dERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerSigmoid():\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        self.A = 1 / (1 + np.exp(-Z))\n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, dERROR, learning_rate):\n",
    "        derivative = self.A * (1 - self.A)\n",
    "        dERROR = dERROR * derivative        \n",
    "        return dERROR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerTanh():\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        self.A = np.tanh(Z)\n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, dERROR, learning_rate):\n",
    "        derivative = 1 - np.square(self.A)\n",
    "        dERROR = dERROR * derivative\n",
    "        return dERROR\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerSoftmax():\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        self.A = np.exp(Z) / np.sum(np.exp(Z), axis=0)\n",
    "        return self.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor_data(n_x,m):\n",
    "    X = np.random.randn(n_x, m)\n",
    "    mask1 = X[0, :] > 0\n",
    "    mask2 = X[1, :] > 0\n",
    "    Y = np.logical_xor(mask1, mask2)\n",
    "    Y = Y.reshape(1, m)\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = xor_data(2,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.add(LayerFC(X.shape[0],2))\n",
    "net.add(LayerTanh())\n",
    "net.add(LayerFC(2, 2))\n",
    "net.add(LayerTanh())\n",
    "net.add(LayerFC(2, 1))\n",
    "net.add(LayerSigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 0.738 Acc: 0.4866666666666667\n",
      "Epoch: 500 Cost: 0.664 Acc: 0.4866666666666667\n",
      "Epoch: 1000 Cost: 0.629 Acc: 0.5766666666666667\n",
      "Epoch: 1500 Cost: 0.589 Acc: 0.7266666666666667\n",
      "Epoch: 2000 Cost: 0.502 Acc: 0.8266666666666667\n",
      "Epoch: 2500 Cost: 0.365 Acc: 0.91\n",
      "Epoch: 3000 Cost: 0.277 Acc: 0.9533333333333334\n",
      "Epoch: 3500 Cost: 0.231 Acc: 0.95\n",
      "Epoch: 4000 Cost: 0.201 Acc: 0.9533333333333334\n",
      "Epoch: 4500 Cost: 0.18 Acc: 0.9566666666666667\n"
     ]
    }
   ],
   "source": [
    "net.train(X, Y, epochs=5000, learning_rate=0.1, multiclass=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    " def gen_3plasma_zones(sample_size, show_dist=False):\n",
    "    \n",
    "    cat_images = np.random.randn(sample_size, 2) + np.array([0, -3])\n",
    "    mouse_images = np.random.randn(sample_size, 2) + np.array([3, 3])\n",
    "    dog_images = np.random.randn(sample_size, 2) + np.array([-3, 3])\n",
    "\n",
    "    X = np.vstack([cat_images, mouse_images, dog_images]).T \n",
    "    Y_temp = np.array([0]*sample_size + [1]*sample_size + [2]*sample_size)\n",
    "    \n",
    "    Y = np.zeros((3, sample_size * 3))\n",
    "    for i in range(sample_size * 3):\n",
    "        Y[Y_temp[i], i] = 1\n",
    "\n",
    "    if show_dist:\n",
    "        plt.scatter(X[0, :], X[1, :], c=Y_temp, cmap='plasma', s=100, alpha=0.6)  \n",
    "        plt.show()\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "X_train_multi, Y_train_multi = gen_3plasma_zones(300, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.add(LayerFC(X_train_multi.shape[0],2))\n",
    "net.add(LayerTanh())\n",
    "net.add(LayerFC(2, 3))\n",
    "net.add(LayerTanh())\n",
    "net.add(LayerFC(3, 3))\n",
    "net.add(LayerSoftmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 0.7829121790772259\n",
      "Epoch: 500 Cost: 0.01502288982540576\n",
      "Epoch: 1000 Cost: 0.007008511693580877\n",
      "Epoch: 1500 Cost: 0.004556002219196872\n",
      "Epoch: 2000 Cost: 0.003371629901704436\n",
      "Epoch: 2500 Cost: 0.0026743124691669252\n",
      "Epoch: 3000 Cost: 0.0022149268110831753\n",
      "Epoch: 3500 Cost: 0.0018894778296183665\n",
      "Epoch: 4000 Cost: 0.001646868743960777\n",
      "Epoch: 4500 Cost: 0.0014590627895047102\n"
     ]
    }
   ],
   "source": [
    "net.train(X_train_multi, Y_train_multi, epochs=5000, learning_rate=0.09, multiclass=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 900)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = net.forward(X)\n",
    "np.argmax(pred, axis = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
